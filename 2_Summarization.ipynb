{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51e69e9-f6de-4224-8da2-c8bb19b0537d",
   "metadata": {},
   "source": [
    "## Summarization Model Setup and Execution\n",
    "\n",
    "In this section, we establish the setup for summarizing parliamentary speeches using the BART model from Hugging Face's transformers library. The BART model, specifically `facebook/bart-large-cnn`, is utilized for generating concise summaries from extended texts. This is particularly useful in contexts like summarizing legislative discussions where the main points need to be distilled efficiently.\n",
    "\n",
    "### Importing Libraries\n",
    "\n",
    "We start by importing necessary Python libraries, including BART tokenizer and model from transformers, tqdm for progress bars, and pandas for data handling.\n",
    "#### Model Reference\n",
    "\n",
    "The summarization task utilizes the BART model, which stands for Bidirectional and Auto-Regressive Transformers. BART is particularly designed for natural language generation, translation, and comprehension tasks. For detailed methodology and insights, refer to the original paper:\n",
    "\n",
    "Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. arXiv preprint arXiv:1910.13461. Available at [http://arxiv.org/abs/1910.13461](http://arxiv.org/abs/1910.13461).\n",
    "\n",
    "This pre-trained model is leveraged in our project to ensure that the summaries generated from the Albanian parliamentary speeches maintain a high level of coherence and factual accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1daeefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a42381ed-7455-4dd8-b95b-ae37891ffc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Instead of from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc4376-d031-43df-bb10-71a1e6bd7123",
   "metadata": {},
   "source": [
    "Testing the model on a single example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0390541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most serious decision he will make is whether the law will be implemented as the sponsor has proposed. An estimate has been made there for the last quarter of 2007. It means, this has to be decided today, even in principle. If we want to be serious in the Assembly, we have to approve that proposal.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary_test(text):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=500, min_length=50, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "text = \"Thank you, Mr. Mayor. The minister cleared it up, partly. The dilemma still remains in the government's decision, and this decision must be made. The most serious decision he will make is whether the law will be implemented as the sponsor has proposed, which should begin to be implemented since the last quarter of this year. So it's got to be the mixers. An estimate has been made there for the last quarter of 2007. It means, this has to be decided today, even in principle. Let the Parliamentary Commission not be allowed that decision but, in principle, to make that decision, not be accepted by the Government's decision. Accept the proposal from the Ministry. Let's start implementation after the adoption, only on that condition. If we want to be serious in the Assembly, we have to approve that proposal. This decision will automatically not be issued because it is proposed that implementation of the bill begin in 2011, according to the Government's proposal, the prime minister. It means to proceed for approval in the Kosovo Assembly. If we don't approve of this decision, then we can go in principle and be serious. If the 2011 clause remains, let another government approve.\"\n",
    "print(generate_summary_test(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60c701-500d-4af5-a386-42df45594e1f",
   "metadata": {},
   "source": [
    " Summarization Function The function `generate_summary` is defined to encode texts into model-readable inputs, generate summaries, and decode these summaries back into readable text. It captures any errors during the process, ensuring that our pipeline can handle unexpected inputs gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8043b6ba-e363-4221-8734-b94349cc3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    try:\n",
    "        # Encode the text into input ids and truncate if necessary without adding any prefix\n",
    "        inputs = tokenizer.encode(text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        # Generate summary ids with constraints\n",
    "        summary_ids = model.generate(inputs, max_length=500, min_length=50, num_beams=4, early_stopping=True)\n",
    "        # Decode the generated ids to text\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fabe457-7dbf-4ab0-ae0b-974be99ec142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "text    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('Data/English_Translated_Speaches.xlsx')\n",
    "# For a single column\n",
    "print(data['text'].dtype)\n",
    "\n",
    "# For multiple specific columns\n",
    "print(data[['text']].dtypes)  # Notice the double brackets for DataFrame slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e5989-f7b5-422c-aaf3-9eafcda16929",
   "metadata": {},
   "source": [
    "### Data Loading and Processing\n",
    "We load our dataset containing English-translated speeches. After ensuring columns are correctly named (stripping any extra whitespace), we apply the summarization function across our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53c4a24d-0ccb-4f33-b5ac-7624568ac22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame after renaming: Index(['text', 'id'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing speeches: 100%|██████████| 1000/1000 [2:54:38<00:00, 10.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization complete and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize_dataset(file_path):\n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Rename the columns to remove any leading or trailing spaces\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    print(\"Columns in DataFrame after renaming:\", df.columns)\n",
    "\n",
    "    # Initialize progress bar\n",
    "    tqdm.pandas(desc=\"Summarizing speeches\")\n",
    "    \n",
    "    # Apply the summary generation function to the 'text' column\n",
    "    df['Summarized_Speech'] = df['text'].progress_apply(generate_summary)\n",
    "\n",
    "    # Check for 'id' column and only proceed if present\n",
    "    if 'id' in df.columns:\n",
    "        output_df = df[['id', 'Summarized_Speech']]\n",
    "        # Save the updated DataFrame to a new Excel file\n",
    "        output_df.to_excel('Summarized_English_Speeches.xlsx', index=False)\n",
    "        return \"Summarization complete and saved.\"\n",
    "    else:\n",
    "        return \"Warning: 'id' column not found. Make sure your Excel file has an 'id' column.\"\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = 'Data/English_Translated_Speeches.xlsx'\n",
    "print(summarize_dataset(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65561f-d04d-4dfa-ad78-b9174c215e22",
   "metadata": {},
   "source": [
    "Finally, we execute our summarization pipeline on the specified file and print the completion status."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
