{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc279d63-64c2-43d8-ab3e-e92eee88b024",
   "metadata": {},
   "source": [
    "## Comparative Analysis of Fine-Tuning vs. Multishot Prompting Techniques for Summarizing Albanian Parliamentary Speeches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae36eab-d450-4e03-b493-ca1047d6f09a",
   "metadata": {},
   "source": [
    "## Introduction to the Project\n",
    "\n",
    "This project investigates the application of Large Language Models (LLMs) in summarizing parliamentary speeches from the Kosovo Parliament, with a focus on speeches in Albanian. The primary goal is to identify the most efficient method for handling this task, considering both effectiveness and cost.\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "1. **Fine-Tuning a Language Model:**\n",
    "   We will fine-tune a pre-trained language model specifically for the task of summarization. This method, while potentially more accurate, is resource-intensive and costly. We aim to evaluate its performance in generating accurate summaries directly from Albanian texts.\n",
    "\n",
    "2. **Exploring Prompt Engineering Techniques:**\n",
    "   In contrast to fine-tuning, prompt engineering requires fewer resources. We will test different techniques to determine their effectiveness compared to fine-tuning:\n",
    "   - **Zero-Shot Learning:** The model attempts to summarize without prior training on summarization.\n",
    "   - **One-Shot Learning:** The model is guided by a single example of a summary to inform its responses.\n",
    "   - **Few-Shot (Multi-Shot) Learning:** The model learns from multiple examples, potentially improving its summarization capabilities.\n",
    "\n",
    "### Comparison Objective:\n",
    "\n",
    "The central aim is to compare these methodologies to discern which provides the best balance between cost and performance. This comparison will help establish whether the investment in fine-tuning is justified or if prompt engineering can achieve comparable results with less expenditure.\n",
    "\n",
    "By exploring these methods, this project contributes to the computational linguistics field by demonstrating how to efficiently process and summarize less-represented languages using advanced linguistic tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25515d0a-7635-4709-af76-50e356cb2239",
   "metadata": {},
   "source": [
    "![Diagram](images/diagram.jpg \"Kosovo Assembly Session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b8eec-1f1c-443c-a335-932e4d0e2f2d",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Creating Labeled Data\n",
    "\n",
    "The primary challenge in our project is the absence of labeled data suitable for training a summarization model directly. As our dataset, the \"Kosovo-Parliament-Transcriptions,\" is primarily unlabeled, our initial task involves generating this crucial labeled dataset.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "The \"Kosovo-Parliament-Transcriptions\" dataset comprises transcripts from speeches delivered by members of the Kosovo Assembly during parliamentary sessions spanning from 2001. This extensive dataset serves as a foundation for research in natural language processing and political discourse analysis.\n",
    "\n",
    "**Data Source:**\n",
    "The transcripts were sourced from the official website of the Kosovo Assembly [Kosovo Assembly](https://kuvendikosoves.org/), capturing both historical and recent parliamentary activities. The raw data were initially in PDF format and were converted to text using OCR technology. The text was subsequently cleaned to correct punctuation and spelling errors. However, users should be aware of potential residual errors due to the complexities of PDF-to-text conversion.\n",
    "\n",
    "**Data Preparation:**\n",
    "The dataset includes multiple languages, reflecting the multilingual nature of the Kosovo Assembly proceedings. To facilitate the processing for this project, additional steps will be taken:\n",
    "\n",
    "- Conduct further quality assurance to rectify any remaining inconsistencies.\n",
    "- Incorporate metadata such as the language of the speech and the political party of the speaker.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `text`: The transcript of the speech.\n",
    "- `speaker`: The name of the speaker.\n",
    "- `date`: The date of the speech.\n",
    "- `id`: A unique identifier for each speech.\n",
    "- `num_tokens`: The number ovo-Parliament-Transcriptions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56072c0-111a-4a95-ac82-b3d8dea844f7",
   "metadata": {},
   "source": [
    "### Library Installation\n",
    "We need to install the `transformers` library to access pre-trained models and tokenizers for our NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fe2829-1024-4669-be73-fec81ec32dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in e:\\binawork\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in e:\\binawork\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in e:\\binawork\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\binawork\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\binawork\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\binawork\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\binawork\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in e:\\binawork\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in e:\\binawork\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\binawork\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\binawork\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\binawork\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\binawork\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in e:\\binawork\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\binawork\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\binawork\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\binawork\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\binawork\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2537362b-f7f4-4274-825a-011413084a95",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the dataset that contains the transcriptions of the parliamentary speeches. We will filter these speeches to focus only on those in Albanian for our summarization task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753af97b-f66d-40a0-8af5-028b510fea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('Data/Kosovo-Parliament-Transcriptions.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58545ed6-7416-44c0-9f41-922fc2e29325",
   "metadata": {},
   "source": [
    "### Language Detection and Filtering\n",
    "Since the dataset contains speeches in multiple languages, we'll detect and filter out only the Albanian speeches. This is crucial as our summarization model will specifically target Albanian language text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6940f3b-c3d6-41f8-86a2-0e791b9622bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure consistent results\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d79770-8fa8-4c08-ab01-495617215984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely detect language\n",
    "def safe_detect(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"Error\"  # In case the text is too short or detection fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f569cba-6187-423e-ac9d-e652616d533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820b521-28d2-4ad3-8a2a-4727915c04de",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Remove any rows with missing values in the 'text' column to ensure our dataset is clean before proceeding with further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17a2154-167c-4768-8111-5790b5a735b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping NaN values:\n",
      "text                 112\n",
      "speaker              835\n",
      "date                   0\n",
      "id                     0\n",
      "num_tokens             0\n",
      "Detected_Language      0\n",
      "dtype: int64\n",
      "\n",
      "After dropping NaN values:\n",
      "text                 0\n",
      "speaker              0\n",
      "date                 0\n",
      "id                   0\n",
      "num_tokens           0\n",
      "Detected_Language    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the initial state of null values\n",
    "print(\"Before dropping NaN values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop rows where the 'text' column is NaN\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "# Optionally, if you also want to ensure that no entries with missing 'speaker' are retained\n",
    "data = data.dropna(subset=['speaker'])\n",
    "\n",
    "print(\"\\nAfter dropping NaN values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data.to_excel('cleaned_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbaafd-025f-4513-a13b-9fc8db3d4747",
   "metadata": {},
   "source": [
    "### Token Count and Speech Filtering\n",
    "Filter out speeches based on token counts to ensure they are suitable for summarization. We remove speeches that are too short to provide valuable summaries or too long for our model to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2356c1eb-982a-4eeb-962b-fe902cc20236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language and count tokens\n",
    "data['Detected_Language'] = data['text'].apply(safe_detect)\n",
    "data['token_count'] = data['text'].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730ae90-ba0e-4c3f-a79d-532d66312481",
   "metadata": {},
   "source": [
    "### Data Sampling\n",
    "Randomly sample a subset of the filtered speeches to create a manageable dataset for model training and evaluation Due to limited computing power we sample our dataset to 1000 spechees..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616bd390-1e09-44ca-971d-2a672103ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is sampled and the result is saved.\n"
     ]
    }
   ],
   "source": [
    "# Filter for only Albanian speeches that meet token requirements\n",
    "filtered_speeches = data[(data['Detected_Language'] == 'sq') & \n",
    "                         (data['token_count'] > 200) & \n",
    "                         (data['token_count'] <= 1024)]\n",
    "\n",
    "# Drop rows where text is NaN or speaker is missing, if necessary\n",
    "filtered_speeches = filtered_speeches.dropna(subset=['text', 'speaker'])\n",
    "\n",
    "# Randomly sample 1000 speeches from the filtered speeches\n",
    "sampled_speeches = filtered_speeches.sample(n=1000, random_state=1)\n",
    "\n",
    "# Save the sampled speeches to a new Excel file\n",
    "sampled_speeches[['text', 'id']].to_excel('Sampled_Alb_Speeches.xlsx', index=False)\n",
    "print(\"Data is sampled and the result is saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e164fa7b-41de-473d-a150-1381e116afcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>Detected_Language</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93774</th>\n",
       "      <td>A jeni dakord ju shefat e grupeve parlamentare...</td>\n",
       "      <td>KRYESUESI-JA</td>\n",
       "      <td>2011-11-11</td>\n",
       "      <td>2011-11-11_22</td>\n",
       "      <td>575</td>\n",
       "      <td>sq</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68250</th>\n",
       "      <td>Unë e di që ka deputetë opozitarë, ka deputetë...</td>\n",
       "      <td>ALBIN KURTI</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>2014-05-07_34</td>\n",
       "      <td>501</td>\n",
       "      <td>sq</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20032</th>\n",
       "      <td>Faleminderit, kryetar! Komisioni për të Drejta...</td>\n",
       "      <td>FJOLLA UJKANI</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>2021-10-19_188</td>\n",
       "      <td>497</td>\n",
       "      <td>sq</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128476</th>\n",
       "      <td>Ju faleminderit z. kryetar. Sikur të kishte qe...</td>\n",
       "      <td>ARDIAN GJINI</td>\n",
       "      <td>2006-06-29</td>\n",
       "      <td>2006-06-29_94</td>\n",
       "      <td>228</td>\n",
       "      <td>sq</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117464</th>\n",
       "      <td>I nderuar deputet, unë të kuptoj se kemi nganj...</td>\n",
       "      <td>KRYESUESI-JA</td>\n",
       "      <td>2008-11-06</td>\n",
       "      <td>2008-11-06_240</td>\n",
       "      <td>701</td>\n",
       "      <td>sq</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        speaker  \\\n",
       "93774   A jeni dakord ju shefat e grupeve parlamentare...   KRYESUESI-JA   \n",
       "68250   Unë e di që ka deputetë opozitarë, ka deputetë...    ALBIN KURTI   \n",
       "20032   Faleminderit, kryetar! Komisioni për të Drejta...  FJOLLA UJKANI   \n",
       "128476  Ju faleminderit z. kryetar. Sikur të kishte qe...   ARDIAN GJINI   \n",
       "117464  I nderuar deputet, unë të kuptoj se kemi nganj...   KRYESUESI-JA   \n",
       "\n",
       "              date              id  num_tokens Detected_Language  token_count  \n",
       "93774   2011-11-11   2011-11-11_22         575                sq          657  \n",
       "68250   2014-05-07   2014-05-07_34         501                sq          563  \n",
       "20032   2021-10-19  2021-10-19_188         497                sq          576  \n",
       "128476  2006-06-29   2006-06-29_94         228                sq          258  \n",
       "117464  2008-11-06  2008-11-06_240         701                sq          801  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c742b-f4dc-4d7e-843f-de1b35b227d6",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "After creating a sampled dataset, our next step involves translating the Albanian speeches into English. This translation is crucial for several reasons:\n",
    "\n",
    "- **Model Compatibility:** Most pre-trained models, especially those in the domain of summarization, are optimized for English. By translating our dataset into English, we can leverage these advanced models more effectively.\n",
    "\n",
    "- **Quality of Summarization:** Accurate summarization depends significantly on the quality of the input data. English, being a widely supported language in NLP tools, ensures that we have access to robust tools and models that can generate reliable summaries.\n",
    "\n",
    "- **Project Requirement:** As part of our project's goal to create a labeled dataset, it is essential to have high-quality summaries. Translating the speeches into English allows us to use state-of-the-art summarization models, which are predominantly trained on English datasets, thereby enhancing the performance and reliability of our summarization outputs.\n",
    "\n",
    "This translation step is integral to preparing our data for the subsequent summarization phase, where we will feed the translated speeches into a pre-trained summarization model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7399aa0-fa28-4f66-9eb7-43dcfc69b043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, now I have a clearer situation. I thank ...</td>\n",
       "      <td>2013-12-19_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deputy Suzan Novobërdali is speaking on behal...</td>\n",
       "      <td>2012-08-31_153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The minister is speaking once again.</td>\n",
       "      <td>2020-11-12_71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr President! We also support this bill.</td>\n",
       "      <td>2013-07-25_138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On behalf of the SLS Parliamentary Group, doe...</td>\n",
       "      <td>2012-03-29_37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text               id\n",
       "0   Yes, now I have a clearer situation. I thank ...    2013-12-19_64\n",
       "1   Deputy Suzan Novobërdali is speaking on behal...   2012-08-31_153\n",
       "2               The minister is speaking once again.    2020-11-12_71\n",
       "3           Mr President! We also support this bill.   2013-07-25_138\n",
       "4   On behalf of the SLS Parliamentary Group, doe...    2012-03-29_37"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_speaches = pd.read_excel('Data/English_Translated_Speaches.xlsx')\n",
    "english_speaches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "570bcca4-ed9e-46a1-b860-28441a768fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Translate Dataset Columns:\n",
      "Index(['text', 'id'], dtype='object')\n",
      "Hugging Face Model Dataset Columns:\n",
      "Index(['text', 'speaker', 'date', 'id', 'num_tokens', 'Detected_Language',\n",
      "       'Translated_Speech', 'Token_Count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "\n",
    "df_google = pd.read_excel('Data/English_Translated_Speaches.xlsx')\n",
    "df_huggingface = pd.read_excel('OLD/Filtered_Translated_Sampled_Alb_Speeches.xlsx')\n",
    "\n",
    "df_google.columns = df_google.columns.str.strip()\n",
    "df_huggingface.columns = df_huggingface.columns.str.strip()\n",
    "\n",
    "# Display the column names of each dataframe\n",
    "print(\"Google Translate Dataset Columns:\")\n",
    "print(df_google.columns)\n",
    "\n",
    "print(\"Hugging Face Model Dataset Columns:\")\n",
    "print(df_huggingface.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05c88595-e6ab-4eb1-85fc-0930239785f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on 'id' to keep only the rows present in both datasets\n",
    "df_merged = pd.merge(df_google, df_huggingface, on='id', suffixes=('_google', '_huggingface'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7f62a86-7a2f-4517-b725-d08b7f77803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample speeches dataset saved to C:\\Users\\nderi\\OneDrive\\Desktop\\anakondabina\\sample_speeches_for_manual_translation.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Select a few speeches for simplicity\n",
    "df_sample = df_merged.head(6)\n",
    "\n",
    "# Create a new DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'ID': df_sample['id'],\n",
    "    'Original': df_sample['text_google'],  # Assuming 'text_google' has the original text\n",
    "    'Google_Translate': df_sample['text_google'],\n",
    "    'Hugging_Face': df_sample['Translated_Speech']\n",
    "})\n",
    "\n",
    "# Save this DataFrame to a new Excel file for manual ground truth addition\n",
    "output_file_path = r'C:\\Users\\nderi\\OneDrive\\Desktop\\anakondabina\\sample_speeches_for_manual_translation.xlsx'\n",
    "comparison_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Sample speeches dataset saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fce131a2-12fc-412f-a8bf-30ba955248d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for Google Translate: 0.45\n",
      "Average BLEU score for Hugging Face model: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Load the updated dataset with manual translations\n",
    "df_ground_truth = pd.read_excel(('sample_speeches_for_manual_translation.xlsx'))\n",
    "\n",
    "# Ensure the columns are named correctly\n",
    "df_ground_truth.columns = df_ground_truth.columns.str.strip()\n",
    "\n",
    "# Calculate BLEU scores to compare translations with ground truth\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = reference.split()\n",
    "    candidate_tokens = candidate.split()\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "# Calculate BLEU scores for each row\n",
    "df_ground_truth['BLEU_Google'] = df_ground_truth.apply(lambda row: calculate_bleu(row['Original'], row['Google_Translate']), axis=1)\n",
    "df_ground_truth['BLEU_Hugging_Face'] = df_ground_truth.apply(lambda row: calculate_bleu(row['Original'], row['Hugging_Face']), axis=1)\n",
    "\n",
    "# Calculate average BLEU scores\n",
    "average_bleu_google = df_ground_truth['BLEU_Google'].mean()\n",
    "average_bleu_hugging_face = df_ground_truth['BLEU_Hugging_Face'].mean()\n",
    "\n",
    "print(f\"Average BLEU score for Google Translate: {average_bleu_google:.2f}\")\n",
    "print(f\"Average BLEU score for Hugging Face model: {average_bleu_hugging_face:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2a643-bb8d-442d-a721-900bf561d7a2",
   "metadata": {},
   "source": [
    "### Evaluation of Translations\n",
    "\n",
    "The evaluation of translation quality plays a crucial role in our project. We employed the **BLEU score metric** to assess the performance of our translation models. The results of our analysis are as follows:\n",
    "\n",
    "- **Google Translate**: Average BLEU score of **0.45**\n",
    "- **Helsinki-NLP opus-mt (Hugging Face model)**: Average BLEU score of **0.30**\n",
    "\n",
    "Based on these scores, **Google Translate demonstrated superior performance** over the Helsinki-NLP opus-mt model. Additionally, a **manual review of the translations** confirmed that Google Translate consistently provided higher quality and more accurate translations. Given these findings, we decided to **proceed with Google Translate** for the subsequent steps of our project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
